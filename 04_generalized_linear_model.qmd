# Generalized Linear Models
<!---# Generalized Linear Models

```{r import}
library(alone)

data(survivalists)
survivalists$podium = survivalists$result <= 3
```

## Generalized how?
The linear regression model fits a straight line through scatterplot data. That's a very restrictive model. The generalized linear model begins to loosen some of the restrictions by allowing the response to have a distribution other than the normal distribution.

## Distributions
Statisticians have defined distributions to describe common kinds of randomness. In regression modeling, we are primarily concerned about the distribution of the response, rather than the features.  Linear regression assumes that the residuals have a distribution that we call "normal" or "Gaussian". But there are other important distributons that are very common in applied scientific research. I'll give you a brief description of the most important distributions here, but you should talk to a statistician and/or read deeply to understand the conditions that are assumed by the response family you've selected for a model.

### Binomial
A very common kind of response is one that puts each observation in a binary class. Examples include: whether a patient survived after receiving a medical treatment, whether a plant successfully germinated, and whether a a target mineral was found in a rock sample. We call this kind of response a binomial response, and it can refer to either a single outcome, or several independent outcomes that are grouped together. An example of the second case would be a measurement of how many students from a class of 30 improved their test score after a lesson.

### Poisson
This is another type of discrete data, which is used for a very common kind of count data. When a count is of independent events with no upper limit, it's often a Poisson response. For example: how many patients were admitted to the hospital in one day with COVID-19, or how many cars drive past an intersection in one hour. There are further considerations to check with a statistician - e.g., if the events aren't independent then you may have a negative binomial response.

### Gamma
A Gamma distribution is for continuous data, which is also true of the normal distribution. But the Gamma distribution can only have positive values (not including zero), and it has greater variability when the values are greater. 


## The `glm()` function
In R, you will usually estimate a generalized linear model by the `glm()` function. Its interface is almost identical to the `lm()` function except it adds a parameter for the distribution, which is called `family`. So an example looks like this:

```{r logistic-regression-example}
logistic_model = glm(podium ~ gender * age, data=survivalists, family='binomial')
summary(logistic_model)
```

The summary reports that there is apparently no significant effect of gender, or age, nor any significant interaction between them for this data.

```{r poisson-regression-example}

```

### Summary of a GLM
To view and interpret a GLM, use the `summary()` function, just as you did for a linear model. The interpretation is extremely similar, with only minor differences that aren't worth belaboring in this introduction.


### Diagnostics for GLMs




--->

```{r hidden-import}
#| include: false
library(dplyr)
```

**Generalized linear models** (GLMs) are used when the response variable has
some distribution other than normal - logistic regression (binary response) is
by far the most commonly used GLM and Poisson regression (count response) is
fairly common, too. In spite of this, there are many similarities between
ordinary linear models and a generalized linear models.

- The distribution of the residuals isn't normal. Several types of residuals can be used for a GLM â€” R defaults to the deviance residual.
- Response variance is no longer assumed equal (when variance depends on mean).
- The linear model predicts a transformed version of the mean, rather than predicting the mean itself (the mean is transformed by what's called a "Link Function").
- Thus, diagnostics have to change.

### Logistic Regression - UC Berkeley Admissions

The `UCBAdmissions` data set is built into R and provides a tabulation of how many men and women applied and were admitted to six departments at UC Berkeley in 1973. Suppose we want to estimate whether male or female applicants were more likely to be admitted. Since each admission decision is binary, we will use a generalized linear model with binomial response to estimate the effect of gender on admission probability. The data are built-in as an array so we need to convert them to a data frame before getting started.

```{r convert-ucb-data}
library(tidyr)
ucb = as.data.frame(UCBAdmissions) |>
  pivot_wider(names_from = Admit, values_from = Freq)

ucb
```

Now fit the binomial regression model for admission probability. By default,
the `glm` function uses logistic regression for a binomial response. The
observations are aggregated by gender and department, so we have to pair the
`Admitted` and `Rejected` columns as the response. If each row represented a
single observation and the `Admitted` column were coded as 0/1 or
`TRUE`/`FALSE`, we could use it alone as the response.

```{r ucb-model}
# estimate a logistic regression model.
ucb_model = glm(cbind(Admitted, Rejected) ~ Dept + Gender, data=ucb, family='binomial')

# plot the diagnostics
layout(matrix(1:4, 2, 2))
plot(ucb_model)

# show the model summary
summary(ucb_model)
```

Diagnostics are especially difficult to interpret for logistic regression models with a small amount of data. These look only OK. According to the model summary, there are significant differences in admission rates by departments and not by gender.

For a logistic regression model, the estimated coefficients are reported as
**log odds ratios**. A log odds ratio greater than zero corresponds to
probability greater than 50%, and vice versa. Both `Dept` and `Gender` are
categorical, so you can figure out the estimated log odds ratio for a
particular combination by adding the relevant rows from the summary table. The
default level of this combination is `Male` and `DeptA`.

<!---This data has often been used to illustrate Simpson's Paradox, because overall men were admitted at a greater rate than women, but this may be because women applied to more selective departments. --->

## Poisson Regression - Effectiveness of Bug Sprays

We conclude with an example using count data as a response. The `InsectSprays` dataset is built in to R so you can import it with the command `data(InsectSprays)`. It has 72 observations of two features. The two features are: the type of insecticide that was applied to a plant, and the other is a count of how many insects were found on the plant. Our goal is to determine whether the different insecticides lead to a consistent difference in the number of insects.

Plotting the data reveals that there are six insecticides in the study, with twelve observations each:

```{r plot-InsectSprays}
#| message: false
#| warnings: false
data(InsectSprays)
plot(InsectSprays)
```

We can see a few more things from the plot. Counts range from zero to 26, and it appears that the insecticides are in two groups: the counts for sprays 3, 4, and 5 are clustered at low counts (ony one count in this group is greater than six), while the counts for sprays 1, 2, and 6 are all at least seven. That suggests that there is a significant difference between the treatments.

We can also see that the counts are more densely clustered for the lesser counts, and more spread out for the greater counts. That's an unequal variance, so the linear model is probably not appropriate. There is a response type specifically for count data that gets more spread out as the average count grows: Poisson. Let's fit a Poisson regression to the insecticide data.

```{r poisson-regression-insect-sprays}
model_spray = glm(count ~ spray, data=InsectSprays, family='poisson')
summary(model_spray)

# make diagnostic plots:
deviance_residuals = residuals(model_spray, type="deviance")
layout(matrix(1:2, 1, 2))
qqnorm(deviance_residuals)
abline(a=0, b=1, lty=3)
plot(fitted(model_spray), deviance_residuals)
```

The diagnostics for this model look pretty good. The Q-Q plot may indicate slightly heavy tails, which would be a sign that the response is overdispersed for the Poisson distribution. A further check is to calculate the mean and variance for each group in the data (this is ony pratical because the model is simple and the number of levels is small). If the data has a Poisson distribution, we expect that the mean and the variance are approximately equal. Of course, real data never perfectly matches this expected relationship.

```{r check-for-overdispersion}
# check the men-variance relationship
group_by(InsectSprays, spray) |>
  summarize(mean = mean(count),
            var = var(count))
```

In most cases, the variance is slightly greater than the mean. Only group F has variance more than twice the mean, which is a reasonable threshold for where to begin to worry.

### Negative-binomial regression
I'd probably leave the `InsectSprays` model as it is for simplicity, but it would also be reasonable to change the model to account for overdispersion. There are a lot of ways to do that, and one classic method is to model the response as a negative-binomial distribution. That requires loading the `MASS` package, which provies the `glm.nb()` function. Here's the result:


```{r poisson-regression-insect-sprays-nb}
# import the MASS package
library(MASS)

#estimate the NB model
model_spray_nb = glm.nb(count ~ spray, data=InsectSprays)
summary(model_spray_nb)

# make diagnostic plots:
deviance_residuals_nb = residuals(model_spray_nb, type="deviance")
layout(matrix(1:2, 1, 2))
qqnorm(deviance_residuals_nb)
abline(a=0, b=1, lty=3)
plot(fitted(model_spray_nb), deviance_residuals_nb)

# compare the models on the basis of AIC
AIC(model_spray, model_spray_nb)
```

The negative-binomial model has a smaller AIC and the Q-Q lies more along the line. We should conclude that the negative-binomial model appears to be a slightly better fit for the data.

