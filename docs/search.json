[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Practical Introduction to Regression Modeling in R",
    "section": "",
    "text": "Overview\n\n\nDescription\nRegression modeling — using input variables to predict or model the value of a response — is widely used in pretty much every field of research. Yet many graduate programs don’t include formal training in statistical modeling, and the DataLab’s office hours indicate widespread anxiety about using regression models in practice. This workshop is intended to help address that anxiety by teaching the fundamentals of using regression modeling in practice. The emphasis is on practice and intuition, with only a small amount of math. This workshop is open to all UC Davis graduate students and postdoctoral scholars. Attendance at both sessions is required. Instruction is in-person and seats are limited. A Zoom link (e.g., broadcast) will be available for those unable to attend who would like to watch live.\n\n\nLearning objectives\nAfter this workshop, learners will be able to: - Understand the differences between linear and generalized linear regression models - Understand the difference between fixed effects and random effects in regression models - Understand how continuous and categorical variables are handled differently in regression modeling software - Implement the above-mentioned model types - Read and interpret regression summary tables - Do diagnostic checks on your regression models"
  },
  {
    "objectID": "01_introduction.html#outline",
    "href": "01_introduction.html#outline",
    "title": "Introduction",
    "section": "Outline",
    "text": "Outline\nThese are the parts of the workshop:\n\nFirst day\n\nLinear model\nCategorical vs. continuous features\n\n\n\nSecond day\n\nGeneralized linear model\nFixed and random effects"
  },
  {
    "objectID": "02_linear_model.html#residuals",
    "href": "02_linear_model.html#residuals",
    "title": "Linear Regression",
    "section": "Residuals",
    "text": "Residuals\nWe have a special term for the difference between the fitted line and the true dots. We call the differences residuals, and there is one per dot. The difference is calculated as the vertical distance, as shown here:\n\n\n\n\n\nObviously, you’d want your model of the response to fit perfectly but there’s no line that would go through all the points."
  },
  {
    "objectID": "02_linear_model.html#how-the-line-is-calculated",
    "href": "02_linear_model.html#how-the-line-is-calculated",
    "title": "Linear Regression",
    "section": "How the line is calculated",
    "text": "How the line is calculated\nA line is totally defined by its slope and intercept (intercept is where the line crosses the y-axis). The math of linear regression is just a way to calculate the slope and intercept of that line, and its intuition is also quite simple. It starts with the goal of minimizing the errors. There is an error for each dot, which is the difference between the line and the dot. To minimize the errors, we need to combine all those numbers into one (otherwise, you might have to worry about what effect a change in “A” has on “B”, etc.) A natural way to combine many numbers into one is to add them together (or take the mean, which is adding together and then dividing by a constant. Since the constant doesn’t change, we can leave it out or not without affecting the location of the minimum.) But there is a problem: errors can be negative (when the model fit is greater than the observed data.) A large negative error would be a good thing for “minimizing” error, but we don’t want that because the error is large. So the errors are squared before adding them together. This is the origin of terms you might have heard, like the sum of squared errors or the mean squared error.\nWe call each dot an “observation”, and each one comes from one row of the spreadsheet. Each column of the spreadsheet is a “feature”. One column is special because it is the column where errors are calculated. We call that column the “response”.\nLet’s say what we mean by a “linear model”. This is an equation that describes some output variable as a linear function of some input variable(s).\nNote that the linear model is a relationship, while linear regression is a method of estimating that relationship. But since it is by far the most common way of estimating that relationship, the terms have come to be used interchangeably.\n\nThe lm() function in R\nThe function to estimate a linear regression model in R is called lm(). We’ll get quite familiar with the function during this workshop. Now let’s use it to estimate the regression line in our distance-to-stop example.\n\nstop_model = lm(dist ~ speed, data=cars)\nsummary(stop_model)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\nThere is a bit of unique R code in the call to lm(): it uses R’s formula syntax. A formula in R has the response variable on the left of a tilde (~) and predictors on the right. You may see it in other contexts but its most common use is to specify the variables of a regression formula. Having used the lm() function to estimate the regression model, we then use the summary() function to inspect the model fit. Let’s dig into the summary() output.\nThe important parts of the summary() results are the Coefficients: and below. The first two parts of the summary() result (Call: and Residuals:) are usually not very interesting. At this point, you probably recognize that the Call: is repeating back the function call that created the model, and the Residuals: section tells you about the size of the residuals.\nStarting with Coefficients: we begin to learn about the model fit. You remember that the linear model fits a straight line to the data. And you might also know that you can describe a line by its slope and intercept, as in \\(y = ax + b\\). In that equation, \\(b\\) is the intercept and \\(a\\) is the slope, also known as the coefficient of \\(x\\). The coefficient of speed functions as the slope of our line, and it is listed in the Estimate column. As you might guess, the intercept of the estimated line is listed as under the Estimate column and the (Intercept) row.\nThe Std. Error column is an estimate of uncertainty in the coefficient estimates. The t value column is just the Estimate divided by the Std. Error, and it is used to calculate the Pr(&gt;|t|) column (better known as the coefficient p-value.)\nThe remaining information (Residual standard error, degrees of freedom, Multiple R-squared, Adjusted R-squared, F-statistic, and p-value) is beyond this introductory workshop. Just know that the p-value reported here is almost useless.\nIn contrast, the coefficient p-values, reported as Pr(&gt;|t|) in the Coefficients: table, are often the main focus of analysis. Making use of these p-values and interpreting the asterisks as indicators of statistical significance depends on proper use of the lm() function. In particular, you must decide which variables to use before fitting a model, and you can only try once - otherwise, the p-values will be biased by peeking at the result before doing the test."
  },
  {
    "objectID": "02_linear_model.html#assumptions-of-linear-regression",
    "href": "02_linear_model.html#assumptions-of-linear-regression",
    "title": "Linear Regression",
    "section": "Assumptions of linear regression",
    "text": "Assumptions of linear regression\nThere are a few assumptions about your data that come with linear regression. Before you can accept the results, you must check these: 1. Linearity: The actual relationship between the features and the response is linear. A trend in the fitted vs. residual plot is evidence that the linearity assumption may be wrong. 2. Check that the residuals have a normal distribution. You can check this via the Q-Q plot, which should have all the dots in an approximately straight line. 3. Constant/equal residual variance: The residuals should have the same variability, also called the scale. Confirm this by the location-scale plot. 4. The residuals must be independent of each other. You can’t actually check this from the data, so you have to think carefully about how the value of one residual might depend upon others (for instance if they are measured at locations that touch, maybe there is something that affects both.) Data collection should be planned in order to have independent responses.\nLet’s check the assumptions on the distance-to-stop model:\n\nlayout(matrix(1:4, 2, 2))\nplot(stop_model)\n\n\n\n\nHere, there is a slight fan-shaed pattern in the Residual v. Fitted plot, and an increasing trend in the Scale-Location plot. Both of these indicate that the variances of the residuals aren’t equal. You can also see the Q-Q plot deviate from the dotted diagonal line, which indicates that the residuals may not be all from an identical normal distribution. The deviations from ideal are pretty minor, and you could probably rely on this model to predict new data. But a more correct model is possible by applying a transformation that mutes the variability of the largest values (often that’s a log transformation, but here a square-root transformation would be ideal.)\n\nstop_model_sqrt = lm(sqrt(dist) ~ speed, data=cars)\nsummary(stop_model_sqrt)\n\n\nCall:\nlm(formula = sqrt(dist) ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0684 -0.6983 -0.1799  0.5909  3.1534 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.27705    0.48444   2.636   0.0113 *  \nspeed        0.32241    0.02978  10.825 1.77e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.102 on 48 degrees of freedom\nMultiple R-squared:  0.7094,    Adjusted R-squared:  0.7034 \nF-statistic: 117.2 on 1 and 48 DF,  p-value: 1.773e-14\n\nlayout(matrix(1:4, 2, 2))\nplot(stop_model_sqrt)"
  },
  {
    "objectID": "02_linear_model.html#multiple-features",
    "href": "02_linear_model.html#multiple-features",
    "title": "Linear Regression",
    "section": "Multiple features",
    "text": "Multiple features\nOur example above has just a single feature to create a model for the response. It is more common to have multiple features, and there really is no limit to how many. However, if the number of features is greater than the number of observations, then we will have problems with the estimation methods. So assume \\(p&lt;n\\).\nWhen there are multiple features, it is no longer possible to draw the relationship as a line through a scatterplot. But everything else works just the same.\n\nWhich features to include?\nA common question is how to decide which features to include in a model. A definitive answer is probably impossible, since well-one model selection (as it is called) is the product of experience and expertise with the system that you’re modeling. In the simplest terms, the correct features for your model are the ones that are relevant to your study. They should be considered carefully before trying to estimate the model, and of course you have to keep in mind assumption (1) for linear models: “The actual relationship between the features and the response is linear.” This is a theory-driven approach to model selection, because you begin with an idea of the model you want to fit, and then tell the computer to estimate it.\nThere are data-driven ways of doing model selection, most of which can be summarized as: try a model and then change it to get a better fit. These approaches are dangerous because they tend to over-fit the training data, which usually makes the model less useful for future data."
  },
  {
    "objectID": "02_linear_model.html#generalized-linear-regression",
    "href": "02_linear_model.html#generalized-linear-regression",
    "title": "Linear Regression",
    "section": "Generalized linear regression",
    "text": "Generalized linear regression\nWe won’t put much emphasis on generalized linear models (GLMs) in this workshop, but they are important and so you might want to know about them. GLMs are used when the response variable has some distribution other than normal - logistic regression (binary response) is by far the most commonly used GLM and Poisson regression (count response) is fairly common, too. In any case, the differences between an ordinary linear model and a generalized linear model are fairly minor.\n\nResponse distribution is different. As a result:\n\n\nResponse variance is no longer assumed equal (when variance depends on mean).\nThe linear model predicts a transformed version of the mean, rather than predicting the mean itself (the mean is transformed by what’s called a “Link Function”).\nThus, diagnostics have to change."
  },
  {
    "objectID": "02_linear_model.html#examples",
    "href": "02_linear_model.html#examples",
    "title": "Linear Regression",
    "section": "Examples",
    "text": "Examples\n\nSeatbelts save lives\nThe UK introduced a law to require seatbelts in January 1983. We have a dataset of the monthly driver fatalities in the UK from 1969 through 1984, and want to estimate what effect the seatbelt law had on driver fatalities. The Seatbelts data set is built into R and can be imported via data(\"Seatbelts\"). The relevant columns are DriversKilled, kms, PetrolPrice, and law.\n\ndata(\"Seatbelts\")\nseatbelt_model = lm(DriversKilled ~ kms + PetrolPrice + law, data = Seatbelts)\nsummary(seatbelt_model)\n\n\nCall:\nlm(formula = DriversKilled ~ kms + PetrolPrice + law, data = Seatbelts)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-50.69 -17.29  -4.05  14.33  60.71 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.015e+02  1.626e+01  12.393  &lt; 2e-16 ***\nkms         -1.223e-03  6.657e-04  -1.838 0.067676 .  \nPetrolPrice -5.683e+02  1.521e+02  -3.738 0.000246 ***\nlaw         -1.189e+01  6.026e+00  -1.973 0.049955 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.87 on 188 degrees of freedom\nMultiple R-squared:  0.201, Adjusted R-squared:  0.1882 \nF-statistic: 15.76 on 3 and 188 DF,  p-value: 3.478e-09\n\nlayout(matrix(1:4, 2, 2))\nplot(seatbelt_model)\n\n\n\n\n\n\nSex of penguins\nHere is an example of a logistic regression model, where we use the species and body mass of penguins to estimate their sex. The data are in the palmerpenguins package, which was created by Allison Horst to compile data on penguins of three species taht were measured at the Palmer Long Term Ecological Research Station in Antarctica.\n\n# load the palmerpenguins package and attach the data\nlibrary(palmerpenguins)\ndata(penguins)\n\nm_penguin_sex = glm(sex ~ species + body_mass_g, data=penguins, family='binomial')\nsummary(m_penguin_sex)\n\n\nCall:\nglm(formula = sex ~ species + body_mass_g, family = \"binomial\", \n    data = penguins)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -2.713e+01  2.998e+00  -9.049   &lt;2e-16 ***\nspeciesChinstrap -2.559e-01  4.293e-01  -0.596    0.551    \nspeciesGentoo    -1.018e+01  1.195e+00  -8.520   &lt;2e-16 ***\nbody_mass_g       7.373e-03  8.141e-04   9.056   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 461.61  on 332  degrees of freedom\nResidual deviance: 212.09  on 329  degrees of freedom\n  (11 observations deleted due to missingness)\nAIC: 220.09\n\nNumber of Fisher Scoring iterations: 6\n\nlayout(matrix(1:4, 2, 2))\nplot(m_penguin_sex)\n\n\n\n\n\n\nBill length of penguins\n\nm_penguin_bill = lm(bill_length_mm ~ flipper_length_mm + species, data=penguins)\nsummary(m_penguin_bill)\n\n\nCall:\nlm(formula = bill_length_mm ~ flipper_length_mm + species, data = penguins)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.662 -1.746  0.028  1.825 12.354 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -2.05859    4.03855  -0.510    0.611    \nflipper_length_mm  0.21505    0.02123  10.129  &lt; 2e-16 ***\nspeciesChinstrap   8.78010    0.39912  21.998  &lt; 2e-16 ***\nspeciesGentoo      2.85689    0.65861   4.338  1.9e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.596 on 338 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.7758,    Adjusted R-squared:  0.7739 \nF-statistic:   390 on 3 and 338 DF,  p-value: &lt; 2.2e-16\n\nlayout(1:4, matrix(2, 2))\nplot(m_penguin_bill)"
  },
  {
    "objectID": "03_categorical_continuous.html#factors",
    "href": "03_categorical_continuous.html#factors",
    "title": "Categorical features",
    "section": "Factors",
    "text": "Factors\nIn R, categorical variables are called factors. Deep down in the machinery of a regression model, factor effects are handled the same way as for continuous features But to fully appreciate this, you have to understand that the factors are coded differently than continuous features. To begin, we should note that linear regression for factor variables is also a kind of scatterplot smoother. Let’s look at an example:\n\nPenguin body mass\nThe plot shows the mass of penguins measured at the Palmer Station LTER site in Antarctica, with the data coming from Allison Horst’s palmerpenguins package.\n\nggplot(penguins_small) + \n  aes(x=species, y=body_mass_g) +\n  geom_point() +\n  ggtitle(\"Body mass of penguins by species\") +\n  ylab(\"Mass (g)\")\n\n\n\n\nLook carefully at the x-axis and you’ll see that the coordinates are species names, not numbers. A line drawn to fit the points would imply that there is a specific order to the species and a specific spacing between them, and that there are some intermediate values where the mass would be somewhere between the known species. But none of those are true.\n\nggplot(penguins_small) +\n  aes(x=species, y=body_mass_g) +\n  geom_point() +\n  geom_abline(intercept=1800, slope=1200) +\n  ggtitle(\"Body mass of penguins by species with linear fit\") +\n  ylab(\"Mass (g)\")\n\n\n\n\nClearly, we need to treat the species as categories, rather than as coordinates along a continuum. Looking at the summary of a regression model gives a clue as to how that works.\n\ncat(\"Mean mass by species:\\n\")\n\nMean mass by species:\n\npenguin_means = with(penguins_small, split(body_mass_g, species)) |&gt;\n  sapply(mean, na.rm=TRUE) |&gt;\n  round(2)\nprint(penguin_means)\n\n   Adelie Chinstrap    Gentoo \n     3580      3660      5150 \n\ncat(\"\\n-----\\n\\nlm summary:\\n\")\n\n\n-----\n\nlm summary:\n\npenguin_lm = lm(body_mass_g ~ species, data=penguins_small)\nsummary(penguin_lm)\n\n\nCall:\nlm(formula = body_mass_g ~ species, data = penguins_small)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-700.0 -147.5   65.0  230.0  550.0 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        3580.0      177.7  20.151 1.28e-10 ***\nspeciesChinstrap     80.0      251.2   0.318    0.756    \nspeciesGentoo      1570.0      251.2   6.249 4.26e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 397.3 on 12 degrees of freedom\nMultiple R-squared:  0.805, Adjusted R-squared:  0.7725 \nF-statistic: 24.77 on 2 and 12 DF,  p-value: 5.494e-05\n\n\nNotice that the fitted value of the (Intercept) is identical to the average mass of an Adelie penguin, and that there are two rows of species effects, instead of the one row that we saw for the continuous effects so far. The values of the Estimate column in those rows are the estimated effects for Chinstrap and Gentoo penguins - and they are equal to the difference between the average mass of those penguins and Adelie penguins.\n\n\nDesign matrix\nThe reason that the results look like this can be made more clear if you look at the way R converts the categories to numbers. The model.matrix() is the function that R uses internally to prepare data for an lm(), but we can call it ourselves. Remember that linear regression works by multiplying each term by a coefficient, adding adding the results together. Here, we have three terms for the three species. Adelie has been automatically selected as the baseline level because it appears first in the data.\n\nmodel.matrix(penguin_lm)\n\n   (Intercept) speciesChinstrap speciesGentoo\n1            1                0             0\n2            1                0             0\n3            1                0             0\n4            1                0             0\n5            1                0             0\n6            1                1             0\n7            1                1             0\n8            1                1             0\n9            1                1             0\n10           1                1             0\n11           1                0             1\n12           1                0             1\n13           1                0             1\n14           1                0             1\n15           1                0             1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$species\n[1] \"contr.treatment\"\n\n\nHere, we have three terms: (Intercept), speciesChinstrap, and speciesGentoo. So to calculate the mass that is estimated for row 12, we will add 1 * (Intercept) + 1 * speciesGentoo. Since the effect (Intercept) is the average mass of the Adelie penguins, the effect speciesGentoo must be the difference between the mean mass of Gentoo penguins and the mean mass of Adelie penguins. Why no term for speciesAdelie? It’s because the fit would then depend on how mass was apportioned between the penguin species and the intercept. You could increase the intercept by 10 grams and reduce all three species estimates by 10 grams and end up with the same model fit, despite different effects. The computer has no way of deciding between those options, because it just optimizing the model fit.\nThere are other possible ways to look at the species effects. One of my favorites is using sum-to-zero contrasts, because then the intercept is the mean treatment effect, rather than being set to one specific baseline treatment effect. In any case, you will see that the estimated effect for each level is identical, despite the difference in\n\npenguin_lm_stz = lm(body_mass_g ~ species, data=penguins_small, contrasts=list(\"species\"=named.contr.sum))\nsummary(penguin_lm_stz)\n\n\nCall:\nlm(formula = body_mass_g ~ species, data = penguins_small, contrasts = list(species = named.contr.sum))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-700.0 -147.5   65.0  230.0  550.0 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             4130.0      102.6  40.265 3.56e-14 ***\nspeciesAdelie-mean      -550.0      145.1  -3.792  0.00257 ** \nspeciesChinstrap-mean   -470.0      145.1  -3.240  0.00709 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 397.3 on 12 degrees of freedom\nMultiple R-squared:  0.805, Adjusted R-squared:  0.7725 \nF-statistic: 24.77 on 2 and 12 DF,  p-value: 5.494e-05\n\nmodel.matrix(penguin_lm_stz)\n\n   (Intercept) speciesAdelie-mean speciesChinstrap-mean\n1            1                  1                     0\n2            1                  1                     0\n3            1                  1                     0\n4            1                  1                     0\n5            1                  1                     0\n6            1                  0                     1\n7            1                  0                     1\n8            1                  0                     1\n9            1                  0                     1\n10           1                  0                     1\n11           1                 -1                    -1\n12           1                 -1                    -1\n13           1                 -1                    -1\n14           1                 -1                    -1\n15           1                 -1                    -1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$species\n          Adelie-mean Chinstrap-mean\nAdelie              1              0\nChinstrap           0              1\nGentoo             -1             -1\n\n\n\n# compare preductions between the models with different contrasts:\npred_df = data.frame(species=c(\"Adelie\", \"Chinstrap\", \"Gentoo\"))\npredict(penguin_lm, pred_df)\n\n   1    2    3 \n3580 3660 5150 \n\npredict(penguin_lm_stz, pred_df)\n\n   1    2    3 \n3580 3660 5150"
  },
  {
    "objectID": "03_categorical_continuous.html#examples",
    "href": "03_categorical_continuous.html#examples",
    "title": "Categorical features",
    "section": "Examples",
    "text": "Examples\nHere is an example of a Poisson regression model.\n\nmodel_spray = glm(count ~ spray, data=InsectSprays, family='poisson')\nsummary(model_spray)\n\n\nCall:\nglm(formula = count ~ spray, family = \"poisson\", data = InsectSprays)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.67415    0.07581  35.274  &lt; 2e-16 ***\nsprayB       0.05588    0.10574   0.528    0.597    \nsprayC      -1.94018    0.21389  -9.071  &lt; 2e-16 ***\nsprayD      -1.08152    0.15065  -7.179 7.03e-13 ***\nsprayE      -1.42139    0.17192  -8.268  &lt; 2e-16 ***\nsprayF       0.13926    0.10367   1.343    0.179    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 409.041  on 71  degrees of freedom\nResidual deviance:  98.329  on 66  degrees of freedom\nAIC: 376.59\n\nNumber of Fisher Scoring iterations: 5\n\nplot(model_spray)"
  },
  {
    "objectID": "04_generalized_linear_model.html#generalized-how",
    "href": "04_generalized_linear_model.html#generalized-how",
    "title": "Generalized Linear Models",
    "section": "Generalized how?",
    "text": "Generalized how?\nThe linear regression model fits a straight line through scatterplot data. That’s a very restrictive model. The generalized linear model begins to loosen some of the restrictions by allowing the response to have a distribution other than the normal distribution."
  },
  {
    "objectID": "04_generalized_linear_model.html#distributions",
    "href": "04_generalized_linear_model.html#distributions",
    "title": "Generalized Linear Models",
    "section": "Distributions",
    "text": "Distributions\nStatisticians have defined distributions to describe common kinds of randomness. In regression modeling, we are primarily concerned about the distribution of the response, rather than the features. Linear regression assumes that the residuals have a distribution that we call “normal” or “Gaussian”. But there are other important distributons that are very common in applied scientific research. I’ll give you a brief description of the most important distributions here, but you should talk to a statistician and/or read deeply to understand the conditions that are assumed by the response family you’ve selected for a model.\n\nBinomial\nA very common kind of response is one that puts each observation in a binary class. Examples include: whether a patient survived after receiving a medical treatment, whether a plant successfully germinated, and whether a a target mineral was found in a rock sample. We call this kind of response a binomial response, and it can refer to either a single outcome, or several independent outcomes that are grouped together. An example of the second case would be a measurement of how many students from a class of 30 improved their test score after a lesson.\n\n\nPoisson\nThis is another type of discrete data, which is used for a very common kind of count data. When a count is of independent events with no upper limit, it’s often a Poisson response. For example: how many patients were admitted to the hospital in one day with COVID-19, or how many cars drive past an intersection in one hour. There are further considerations to check with a statistician - e.g., if the events aren’t independent then you may have a negative binomial response.\n\n\nGamma\nA Gamma distribution is for continuous data, which is also true of the normal distribution. But the Gamma distribution can only have positive values (not including zero), and it has greater variability when the values are greater."
  },
  {
    "objectID": "04_generalized_linear_model.html#the-glm-function",
    "href": "04_generalized_linear_model.html#the-glm-function",
    "title": "Generalized Linear Models",
    "section": "The glm() function",
    "text": "The glm() function\nIn R, you will usually estimate a generalized linear model by the glm() function. Its interface is almost identical to the lm() function except it adds a parameter for the distribution, which is called family. So an example looks like this:\n\nlogistic_model = glm(podium ~ gender * age, data=survivalists, family='binomial')\nsummary(logistic_model)\n\n\nCall:\nglm(formula = podium ~ gender * age, family = \"binomial\", data = survivalists)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    -0.641003   2.322898  -0.276    0.783\ngenderMale     -0.817204   2.558817  -0.319    0.749\nage             0.004875   0.055841   0.087    0.930\ngenderMale:age  0.010940   0.062325   0.176    0.861\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 129.96  on 103  degrees of freedom\nResidual deviance: 128.90  on 100  degrees of freedom\n  (10 observations deleted due to missingness)\nAIC: 136.9\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe summary reports that there is apparently no significant effect of gender, or age, nor any significant interaction between them for this data.\n\nSummary of a GLM\nTo view and interpret a GLM, use the summary() function, just as you did for a linear model. The interpretation is extremely similar, with only minor differences that aren’t worth belaboring in this introduction.\n\n\nDiagnostics for GLMs"
  },
  {
    "objectID": "05_fixed_random_effects.html#fixed-effects",
    "href": "05_fixed_random_effects.html#fixed-effects",
    "title": "Fixed and Random Effects",
    "section": "Fixed effects",
    "text": "Fixed effects\nThe effects we’ve seen so far are often called “fixed effects”. That’s because the treatment is expected to have the same effect if the experiment were repeated. That fixed value is in contrast to “random effects”, which are taken from a random distribution and are expected to be different in a hypothetical repetition of the experiment."
  },
  {
    "objectID": "05_fixed_random_effects.html#random-effects",
    "href": "05_fixed_random_effects.html#random-effects",
    "title": "Fixed and Random Effects",
    "section": "Random effects",
    "text": "Random effects\nReal-world science often sees data grouped according to some known structure of the study. For instance, in an agricultural experiment, the yield of plants from the same field is generally more alike than plants from different fields, even fields that received the same treatment. Why? Maybe the soil was generally better quality in one field, or one got more rain than the other, or the treatments weren’t applied perfectly consistently between fields.\nIn order to account for factors like these, agricultural experiments often divide a field into plots, and apply different treatments in each plot. In that case, the yields from the same plot are more alike than from other plots, even on the same field. But they are generally more alike within the same field than plots from different fields. This is an example of hierarchical data: there are several individual measurements of yield from plants in each plot. We say that the measurements are nested within the plots, and meanwhile the plots are nested within fields."
  },
  {
    "objectID": "05_fixed_random_effects.html#random-effects-1",
    "href": "05_fixed_random_effects.html#random-effects-1",
    "title": "Fixed and Random Effects",
    "section": "Random effects",
    "text": "Random effects\nRandom effects are a way to account for variation in data that arises due to grouping or clustering of observations. They are used in regression models when there is a hierarchy or structure in the data, and the observations within each group are expected to be correlated.\nFor example, in a study involving students from different schools, random effects can be used to capture the inherent differences between schools, which may affect the dependent variable (e.g., test scores). Random effects are modeled as deviations from the overall population-level effects."
  },
  {
    "objectID": "05_fixed_random_effects.html#using-random-effects-in-regression",
    "href": "05_fixed_random_effects.html#using-random-effects-in-regression",
    "title": "Fixed and Random Effects",
    "section": "Using random effects in regression",
    "text": "Using random effects in regression\nThere are several R packages that implement random effects in regression. lme4 is the most-used and brms is the Bayesian equivalent (which also offers some great features that aren’t available in lme4.) Both of them write the model in just about the same way as the lm() and glm() functions that you’ve already seen. The only difference is that the random effects need to be specified using a special notation: they are written as two parts wrapped in parentheses. The first part indicates the effect that changes with the grouping factor, and the second part indicates what variable is the grouping factor.\n\nre_model = lmer(Reaction ~ Days + (1|Subject), data=sleepstudy)\n\n\n# Predict the fitted values from the mixed-effects model\nsleepstudy$fitted_reaction &lt;- predict(re_model)\n\n# Create a scatterplot of fitted reaction times against Days\nggplot(sleepstudy) +\n  aes(x = Days, y = fitted_reaction, color = Subject) +\n   # geom_point() +\n   geom_line() +\n   labs(title = \"Fitted Reaction Time vs. Days by Subject\",\n        x = \"Days\",\n        y = \"Fitted Reaction Time (ms)\") +\n  geom_point(data=sleepstudy, mapping=aes(x=Days, y=Reaction, color=Subject))\n\n\n\n\n\nRandom intercepts and slopes\nThe most common kind of random effect is a random intercept. That means the effect of a grouping level is a consistent adjustment (increase or decrease) to the response. But random effects can be more complicated, such random slopes - where the effect of a continuous variable changes according to the grouping variable. Here’s an example using the sleep study data: the difference is that now the random effect of subject is not only that each subject was has somewhat quicker or slower reflexes that change by the same amount each day. Instead, each subject has somewhat quicker or slower reflexes that change by a personally unique amount each day (meanwhile, we have still estimated the fixed effect of days - which is the amount that an average person’s reaction time slowed down with each day of sleep deprivation.\n\nrandom_slope_model = lmer(Reaction ~ Days + (1 + Days|Subject), data=sleepstudy)\n\nLooking at the plot of random effects, we see that the people with the quickest reactions also were less affected by sleep deprivation (lowest lines have the flattest slopes), and the people with the slowest reactions also were most affected by sleep deprivation (highest lines have the steepest slopes.)\n\n# Predict the fitted values from the mixed-effects model\nsleepstudy$random_slope_fitted &lt;- predict(random_slope_model)\n\n# Create a scatterplot of fitted reaction times against Days\nggplot(sleepstudy) +\n  aes(x = Days, y = random_slope_fitted, color = Subject) +\n   # geom_point() +\n   geom_line() +\n   labs(title = \"Fitted Reaction Time vs. Days by Subject\",\n        x = \"Days\",\n        y = \"Fitted Reaction Time (ms)\") +\n  geom_point(data=sleepstudy, mapping=aes(x=Days, y=Reaction, color=Subject))\n\n\n\n\nThis is also seen in the model summary, where there is a (slightly) positive correlation between the random slope and random intercept.\n\nsummary(random_slope_model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + (1 + Days | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1743.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9536 -0.4634  0.0231  0.4634  5.1793 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n Subject  (Intercept) 612.10   24.741       \n          Days         35.07    5.922   0.07\n Residual             654.94   25.592       \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  251.405      6.825  36.838\nDays          10.467      1.546   6.771\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.138"
  },
  {
    "objectID": "05_fixed_random_effects.html#when-to-use-fixed-vs-random-effects",
    "href": "05_fixed_random_effects.html#when-to-use-fixed-vs-random-effects",
    "title": "Fixed and Random Effects",
    "section": "When to use fixed vs random effects",
    "text": "When to use fixed vs random effects\nThere isn’t a single, definitive check that will tell you when to use random or fixed effects. I have come to think of random effects as a form of “partial pooling” - a way to do something between te extremes of either treating each observation as independent, or fully pooling all observations of each group by only using group means as data. With random effects, the amount of pooling is adaptive: where the differences between groups are large compared to the difference between observations within a group, then the random effect for group will have a big influence. But where the differences between individual observations is large compared to"
  },
  {
    "objectID": "05_fixed_random_effects.html#examples",
    "href": "05_fixed_random_effects.html#examples",
    "title": "Fixed and Random Effects",
    "section": "Examples:",
    "text": "Examples:\n\nOrange trees\nFor this example, we will look at the growth of orange trees. The data is in the built-in dataset Orange, which you can import via \\(data(Orange)\\). There are three columns: Tree, which is an ID of which orange tree is being measured; age, which is the number of days since Deember 31, 1969; and circumference, which is the circumference of the tree’s trunk (in mm). There are five trees with seven observations each, so 35 rows of data. First, plot the data:\n\n# plot the orange tree data\nggplot(Orange) +\n  aes(x=age, y=circumference, color=Tree) +\n  geom_point()\n\n\n\n\nThis looks like (fairly) linear growth with the same intercept (all the trees appear to have circumference zero at day zero) but slightly different growth rates. So let’s use a random slope model.\n\nmodel_orange = lmer(circumference ~ (age - 1 | Tree), data=Orange)\nsummary(model_orange)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: circumference ~ (age - 1 | Tree)\n   Data: Orange\n\nREML criterion at convergence: 289.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.9824 -0.5130 -0.0082  0.7997  1.6243 \n\nRandom effects:\n Groups   Name Variance  Std.Dev.\n Tree     age    0.01172  0.1082 \n Residual      100.93833 10.0468 \nNumber of obs: 35, groups:  Tree, 5\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)   17.913      3.642   4.918"
  }
]