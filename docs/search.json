[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Practical Introduction to Regression Modeling in R",
    "section": "",
    "text": "Overview\n\n\nDescription\nRegression modeling — using input variables to predict or model the value of a response — is widely used in pretty much every field of research. Yet many graduate programs don’t include formal training in statistical modeling, and the DataLab’s office hours indicate widespread anxiety about using regression models in practice. This workshop is intended to help address that anxiety by teaching the fundamentals of using regression modeling in practice. The emphasis is on practice and intuition, with only a small amount of math. This workshop is open to all UC Davis graduate students and postdoctoral scholars. Attendance at both sessions is required. Instruction is in-person and seats are limited. A Zoom link (e.g., broadcast) will be available for those unable to attend who would like to watch live.\n\n\nLearning objectives\nAfter this workshop, learners will be able to: - Understand the differences between linear and generalized linear regression models - Understand the difference between fixed effects and random effects in regression models - Understand how continuous and categorical variables are handled differently in regression modeling software - Implement the above-mentioned model types - Read and interpret regression summary tables - Do diagnostic checks on your regression models"
  },
  {
    "objectID": "01_introduction.html#outline",
    "href": "01_introduction.html#outline",
    "title": "Introduction",
    "section": "Outline",
    "text": "Outline\nThese are the parts of the workshop:\n\nFirst day\n\nLinear model\nCategorical vs. continuous features\n\n\n\nSecond day\n\nGeneralized linear model\nFixed and random effects"
  },
  {
    "objectID": "02_linear_model.html#residuals",
    "href": "02_linear_model.html#residuals",
    "title": "Linear Regression",
    "section": "Residuals",
    "text": "Residuals\nWe have a special term for the difference between the fitted line and the true dots. We call the differences residuals, and there is one per dot. The difference is calculated as the vertical distance, as shown here:\nObviously, you’d want your model of the response to fit perfectly, but that’s generally not possible in the real world. So we have to accept the best fit that we can accomplish."
  },
  {
    "objectID": "02_linear_model.html#how-the-line-is-calculated",
    "href": "02_linear_model.html#how-the-line-is-calculated",
    "title": "Linear Regression",
    "section": "How the line is calculated",
    "text": "How the line is calculated\nA line is totally defined by its slope and intercept (intercept is where the line crosses the y-axis). The math of linear regression is just a way to calculate the slope and intercept of that line, and its intuition is also quite simple. It starts with the goal of minimizing the errors. There is an error for each dot, which is the difference between the line and the dot. To minimize the errors, we need to combine all those numbers into one (otherwise, you might have to worry about what effect a change in “A” has on “B”, etc.) A natural way to combine many numbers into one is to add them together (or take the mean, which is adding together and then dividing by a constant. Since the constant doesn’t change, we can leave it out or not without affecting the location of the minimum.) But there is a problem: errors can be negative (when the model fit is greater than the observed data.) A large negative error would be a good thing for “minimizing” error, but we don’t want that because the error is large. So the errors are squared before adding them together. This is the origin of terms you might have heard, like the sum of squared errors or the mean squared error.\nWe call each dot an “observation”, and each one comes from one row of the spreadsheet. Each column of the spreadsheet is a “feature”. One column is special because it is the column where errors are calculated. We call that column the “response”.\nLet’s say what we mean by a “linear model”. This is an equation that describes some output variable as a linear function of some input variable(s). ***Note that the linear model is a relationship, while linear regression is a method of estimating that relationship. But since it is by far the most common way of estimating that relationship, the terms have come to be used interchangeably."
  },
  {
    "objectID": "02_linear_model.html#multiple-features",
    "href": "02_linear_model.html#multiple-features",
    "title": "Linear Regression",
    "section": "Multiple features",
    "text": "Multiple features\nOur exmple above has just a single feature to create a model for the response. It is ore commong to have multiple features, and there really is no limit to how many. However, if the number of features is greater than the number of observations, then we will have problems with the estimation methods. So assume \\(p&lt;n\\).\nWhen there are multiple features, it is no longer possible to draw the relationship as a line trhough a scatterplot. But everything else works just the same.\n\nThe lm() function in R\nThe function to estimate a linear regression model in R is called lm(). We’ll get quite familiar with the function during this workshop."
  },
  {
    "objectID": "02_linear_model.html#assumptions-of-linear-regression",
    "href": "02_linear_model.html#assumptions-of-linear-regression",
    "title": "Linear Regression",
    "section": "Assumptions of linear regression",
    "text": "Assumptions of linear regression\nThere are a few assumptions about your data that come with linear regression. Before you can accept the results, you must check these: 1. Linearity: The actual relationship between the features and the response is linear. A trend in the fitted vs. residual plot is evidence that the linearity assumption may be wrong. 2. Check that the residuals have a normal distribution. You can check this via the Q-Q plot, which should have all the dots in an approximately straight line. 3. Constant/equal residual variance: The residuals should have the same variability, also called the scale. Confirm this by the location-scale plot. 4. The residuals must be independent of each other. You can’t easily check this from the data, so you have to think carefully about how the value of one residual might depend upon others (for instance if they are measured at locations that touch, maybe there is something that affects both.)"
  },
  {
    "objectID": "03_categorical_continuous.html#factors",
    "href": "03_categorical_continuous.html#factors",
    "title": "Categorical features",
    "section": "Factors",
    "text": "Factors\nIn R, categorical variables are called factors. Deep down in the machinery of a regression model, factor effects are handled the same way as for continuous features But to fully appreciate this, you have to understand that the factors are coded differently than continuous features. To begin, we should note that linear regression for factor variables is also a kind of scatterplot smoother. Let’s look at an example:\n\nPenguin body mass\nThe plot shows the mass of penguins measured at the Palmer Station LTER site in Antarctica, with the data coming from Allison Horst’s palmerpenguins package.\n\nggplot(penguins_small) + \n  aes(x=species, y=body_mass_g) +\n  geom_point() +\n  ggtitle(\"Body mass of penguins by species\") +\n  ylab(\"Mass (g)\")\n\n\n\n\nLook carefully at the x-axis and you’ll see that the coordinates are species names, not numbers. A line drawn to fit the points would imply that there is a specific order to the species and a specific spacing between them, and that there are some intermediate values where the mass would be somewhere between the known species. But none of those are true.\n\nggplot(penguins_small) +\n  aes(x=species, y=body_mass_g) +\n  geom_point() +\n  geom_abline(intercept=1800, slope=1200) +\n  ggtitle(\"Body mass of penguins by species with linear fit\") +\n  ylab(\"Mass (g)\")\n\n\n\n\nClearly, we need to treat the species as categories, rather than as coordinates along a continuum. Looking at the summary of a regression model gives a clue as to how that works.\n\ncat(\"Mean mass by species:\\n\")\n\nMean mass by species:\n\npenguin_means = with(penguins_small, split(body_mass_g, species)) |&gt;\n  sapply(mean, na.rm=TRUE) |&gt;\n  round(2)\nprint(penguin_means)\n\n   Adelie Chinstrap    Gentoo \n     3580      3660      5150 \n\ncat(\"\\n-----\\n\\nlm summary:\\n\")\n\n\n-----\n\nlm summary:\n\npenguin_lm = lm(body_mass_g ~ species, data=penguins_small)\nsummary(penguin_lm)\n\n\nCall:\nlm(formula = body_mass_g ~ species, data = penguins_small)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-700.0 -147.5   65.0  230.0  550.0 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        3580.0      177.7  20.151 1.28e-10 ***\nspeciesChinstrap     80.0      251.2   0.318    0.756    \nspeciesGentoo      1570.0      251.2   6.249 4.26e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 397.3 on 12 degrees of freedom\nMultiple R-squared:  0.805, Adjusted R-squared:  0.7725 \nF-statistic: 24.77 on 2 and 12 DF,  p-value: 5.494e-05\n\n\nNotice that the fitted value of the (Intercept) is identical to the average mass of an Adelie penguin, and that there are two rows of species effects, instead of the one row that we saw for the continuous effects so far. The values of the Estimate column in those rows are the estimated effects for Chinstrap and Gentoo penguins - and they are equal to the difference between the average mass of those penguins and Adelie penguins.\n\n\nDesign matrix\nThe reason that the results look like this can be made more clear if you look at the way R converts the categories to numbers. The model.matrix() is the function that R uses internally to prepare data for an lm(), but we can call it ourselves. Remember that linear regresson works by multiplying each term by a coefficient, adding adding the results together. Here, we have three terms for the three species. Adelie has been automatically selected as the baseline level because it appears first in the data.\n\nmodel.matrix(penguin_lm)\n\n   (Intercept) speciesChinstrap speciesGentoo\n1            1                0             0\n2            1                0             0\n3            1                0             0\n4            1                0             0\n5            1                0             0\n6            1                1             0\n7            1                1             0\n8            1                1             0\n9            1                1             0\n10           1                1             0\n11           1                0             1\n12           1                0             1\n13           1                0             1\n14           1                0             1\n15           1                0             1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$species\n[1] \"contr.treatment\"\n\n\nHere, we have three terms: (Intercept), speciesChinstrap, and speciesGentoo. So to calculate the mass that is estimated for row 160, we will add 1 * (Intercept) + 1 * speciesGentoo. Since the effect (Intercept) is the average mass of the Adelie penguins, the effect speciesGentoo must be the difference between the mean mass of Gentoo penguins and the mean mass of Adelie penguins. Why no term for speciesAdelie? It’s because the fit would then depend on how mass was apportioned between the penguin species and the intercept. You could increase the intercept by 10 grams and reduce all three species estimates by 10 grams, and end up with different effects but the same model fit. The computer has no way of deciding between those options, so it must avoid that situation.\nThere are other possible ways to look at the species effects."
  },
  {
    "objectID": "04_generalized_linear_model.html#generalized-how",
    "href": "04_generalized_linear_model.html#generalized-how",
    "title": "Generalized Linear Models",
    "section": "Generalized how?",
    "text": "Generalized how?\nThe linear regression model fits a straight line through scatterplot data. That’s a very restrictive model. The generalized linear model begins to loosen some of the restrictions by allowing the response to have a distribution other than the normal distribution."
  },
  {
    "objectID": "04_generalized_linear_model.html#distributions",
    "href": "04_generalized_linear_model.html#distributions",
    "title": "Generalized Linear Models",
    "section": "Distributions",
    "text": "Distributions\nStatisticians have defined distributions to describe common kinds of randomness. In regression modeling, we are primarily concerned about the distribution of the response, rather than the features. Linear regression assumes that the residuals have a distribution that we call “normal” or “Gaussian”. But there are other important distributons that are very common in applied scientific research. I’ll give you a brief description of the most important distributions here, but you should talk to a statistician and/or read deeply to understand the conditions that are assumed by the response family you’ve selected for a model.\n\nBinomial\nA very common kind of response is one that puts each observation in a binary class. Examples include: whether a patient survived after receiving a medical treatment, whether a plant successfully germinated, and whether a a target mineral was found in a rock sample. We call this kind of response a binomial response, and it can refer to either a single outcome, or several independent outcomes that are grouped together. An example of the second case would be a measurement of how many students from a class of 30 improved their test score after a lesson.\n\n\nPoisson\nThis is another type of discrete data, which is used for a very common kind of count data. When a count is of independent events with no upper limit, it’s often a Poisson response. For example: how many patients were admitted to the hospital in one day with COVID-19, or how many cars drive past an intersection in one hour. There are further considerations to check with a statistician - e.g., if the events aren’t independent then you may have a negative binomial response.\n\n\nGamma\nA Gamma distribution is for continuous data, which is also true of the normal distribution. But the Gamma distribution can only have positive values (not including zero), and it has greater variability when the values are greater."
  },
  {
    "objectID": "04_generalized_linear_model.html#the-glm-function",
    "href": "04_generalized_linear_model.html#the-glm-function",
    "title": "Generalized Linear Models",
    "section": "The glm() function",
    "text": "The glm() function\nIn R, you will usually estimate a generalized linear model by the glm() function. Its iterface is almost identical to the lm() function except it adds a parameter for the distribution, which is called family. So an example looks like this:\n\nmy_glm = glm(podium ~ gender * age, data=survivalists)\n\n\nSummary of a GLM\nTo view and interpret a GLM, use the summary() function, just as you did for a linear model. The interpretation is extremely similar, with only minor differences that aren’t worth belaboring in this introduction.\n\n\nDiagnostics for GLMs"
  }
]